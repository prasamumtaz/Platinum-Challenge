{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc1d8f3-c76a-44eb-8362-0b3b43808316",
   "metadata": {},
   "source": [
    "# Make A Model from Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41504bcb-ce03-43d7-bb8c-859d0d1a0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07f31-b3f7-4ac5-8928-9edbcfe86871",
   "metadata": {},
   "source": [
    "## Import, Clean, Drop Duplicate from Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db517c4-a272-4fd7-8452-f0444a95762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71058335-ed42-4e0d-89ec-b2bd45800911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(r\"E:\\BINAR\\Platinum-Challenge\\Dataset\\train_preprocess.tsv.txt\", sep='\\t', header=None)\n",
    "df_train = df_train.rename(columns={0: 'text', 1: 'label'})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1df0b4-3a10-4ca4-a886-98648cfcbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb0d3c-d66a-4e41-b35c-619f30175afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577dbdd-fb0c-450e-96c4-36f4b10b9dd8",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4ecc0-c5af-484f-922f-fb24d09e2ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fucntion to Clean tweet data\n",
    "def Clean(text):\n",
    "    #lowercase for every word\n",
    "    text = text.lower()\n",
    "\n",
    "    #Clean Pattern\n",
    "    #remove USER\n",
    "    text = re.sub(r'user', ' ', text)\n",
    "    #remove 'RT'\n",
    "    text = re.sub(r'rt', ' ', text)\n",
    "    #remove 'URL'\n",
    "    text = re.sub(r'url', ' ', text)\n",
    "    #remove HTTPS\n",
    "    text = re.sub(r'https', ' ', text)\n",
    "    #remove HTTP\n",
    "    text = re.sub(r'http', ' ', text)\n",
    "    #remove &amp\n",
    "    text = re.sub(r'&amp', ' ', text)\n",
    "\n",
    "    #Clean_Unnecessary_Character\n",
    "    #remove \\n or every word afte '\\' with space\n",
    "    text = re.sub(r'\\\\+[a-zA-Z0-9]+', ' ', text)\n",
    "    #remove text emoji\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]{2,}|:[a-zA-Z0-9]{0,}', ' ', text)\n",
    "    #remove all unnecessary character \n",
    "    text = re.sub(r'[^0-9a-zA-Z\\s]+', ' ', text)\n",
    "    #remove all number\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    #remove extra space\n",
    "    text = re.sub(r'  +', ' ', text)\n",
    "    #remove space at the start or the end of string\n",
    "    text = re.sub(r'^ +| +$', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#tokenization Function\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "#import file new_kamusalay.csv\n",
    "kamus_alay = pd.read_csv(r\"E:\\BINAR\\Binar-Gold-Challenge\\Dataset\\new_kamusalay.csv\", \n",
    "                         encoding = 'ISO-8859-1', header = None)\n",
    "kamus_alay = kamus_alay.rename(columns={0: 'kata alay', 1: 'arti kata'})\n",
    "\n",
    "#Create dictionary from kamus_alay\n",
    "kamus_alay_dict = dict(zip(kamus_alay['kata alay'], kamus_alay['arti kata']))\n",
    "\n",
    "#normalization function to convert every word tha contain 'kata alay' to 'arti kata'\n",
    "def normalization(text):\n",
    "    newlist = []\n",
    "    for word in text:\n",
    "        if word in kamus_alay_dict:\n",
    "            text = kamus_alay_dict[word]\n",
    "            newlist.append(text)\n",
    "        else:\n",
    "            text = word\n",
    "            newlist.append(text)\n",
    "    return newlist\n",
    "\n",
    "#remove stopwords\n",
    "\n",
    "#list stopword from NLTK\n",
    "stopword_list = ['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', \n",
    "                 'dia', 'dua', 'ia','ia', 'seperti', 'jika', 'sehingga', 'kembali', 'dan', \n",
    "                 'ini', 'karena', 'kepada', 'oleh', 'saat', 'sementara', 'setelah', 'kami', \n",
    "                 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', \n",
    "                 'ketika', 'adalah', 'itu', 'dalam', 'bahwa', 'atau', 'kita', 'dengan', 'akan', \n",
    "                 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', \n",
    "                 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', \n",
    "                 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', \n",
    "                 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', \n",
    "                 'tetapi', 'apakah', 'kecuali', 'sebab', 'seolah', 'seraya', 'seterusnya', 'dsb', 'dst', \n",
    "                 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'mari', 'nanti', 'oh', 'ok', 'setiap', \n",
    "                 'sesuatu','saja', 'toh', 'walau', 'amat', 'apalagi', 'dengan', 'bahwa', 'oleh']\n",
    "\n",
    "stopword_list.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       'gue', 'yah', 'kayak'])\n",
    "\n",
    "stopword_list = set(stopword_list)\n",
    "\n",
    "#remove stopword function\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword_list]\n",
    "    return text\n",
    "\n",
    "#Find NUll 'String' Value \n",
    "def clean_non_existed(text):\n",
    "    if text == '':\n",
    "        return None\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "#function to run all the function\n",
    "def clean_data(text):\n",
    "    text = Clean(text)\n",
    "    text = tokenization(text)\n",
    "    text = normalization(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join(text)\n",
    "    text = clean_non_existed(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2bf28-ca8e-4b25-a2f6-e2bc5f1094b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_clean'] = df_train.text.apply(clean_data)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd1777-0812-4f9e-8364-5db50564c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9db268-05cb-466d-9133-320794215057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('positive: ', len(df_train[df_train['label'] == 'positive']))\n",
    "print('negative: ', len(df_train[df_train['label'] == 'negative'])) \n",
    "print('neutral: ', len(df_train[df_train['label'] == 'neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92310f-4938-4c30-b3fd-37c3bf52f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie([6416, 3436, 1148], labels= ['positive', 'negative', 'neutral'], autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf3824-1793-49b2-a6b0-eea8dbdb6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b1df0-b2da-4b2f-bac8-67aa6e21ed24",
   "metadata": {},
   "source": [
    "## Cek duplikat & Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5e81b-b3bf-492a-892e-56ba0e36d1b7",
   "metadata": {},
   "source": [
    "### Duplikat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a5e68-c18b-485a-986e-1d1813bbc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate check\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cdd838-9a4d-41a5-9bc9-8c576492be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate\n",
    "df_train = df_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cf994-a7ad-44fc-9e7b-121902d5a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex\n",
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6c074-022b-4998-a2ee-0669962e5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate check\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c92b2-b8ff-4a44-afb8-683e22da4277",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9c215-5e7b-4df3-8be2-8122cbb7ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef13abe-9d37-47b7-a91e-b42cd8ff87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1bb15-3278-4bba-9b14-2fd0b84db687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dd804-5420-4d27-b530-c491df87635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb316693-eefa-4fd9-94fd-fb12f165eadc",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290e1a6-3f7c-4ac5-9350-98cbdd67f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fb3ec-0060-4ab3-9e2b-b93c5e3104f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = df_train[['text_clean']]\n",
    "labels = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f6539-5541-4644-b304-ab126b956337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_text, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca510bc-e4a9-4675-8769-dc794b49fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Data: ', len(X_train))\n",
    "print('Test Data: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547df71-c072-4c5c-9559-1b61525b9005",
   "metadata": {},
   "source": [
    "### Train Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6236f-7637-4820-935c-5251a7b02a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082fe97-971e-43b2-89bd-122241efcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8e6a5-62cd-4ccb-a1c0-7c102910db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROS = RandomOverSampler()\n",
    "X_train_ros, y_train_ros = ROS.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d57df-386d-4aa7-9859-14c60dbc845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f04332-ea06-4885-b587-921ef3fd2b14",
   "metadata": {},
   "source": [
    "## Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64bf13-3b4f-4e2c-b98c-bec5212c623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert X_train_text into list form\n",
    "Train_preprocessed = X_train_ros.text_clean.tolist()\n",
    "Train_preprocessed[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8f52e-aa0a-474d-84b5-b5a406d3c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert X_test_text into list form\n",
    "Test_preprocessed = X_test.text_clean.tolist()\n",
    "Test_preprocessed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b02cb-a65d-450a-9b21-69ab1694e35b",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7b0da-940d-42d4-b4f6-976c9ea8b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed6a50-0e5e-4012-a427-78938429c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Proses Feature Extraction\n",
    "count_vect = TfidfVectorizer()\n",
    "count_vect.fit_transform(Train_preprocessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748368ec-1eb6-4424-8c8c-b60e6307302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inisiasi value\n",
    "X_train = count_vect.fit_transform(Train_preprocessed)\n",
    "print(\"Feature Extraction For Train Data Has Successfully Completed\")\n",
    "\n",
    "X_test = count_vect.transform(Test_preprocessed)\n",
    "print(\"Feature Extraction For Test Data Has Successfully Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489d517-2d39-4c9d-8b9f-2d0a0bc0c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vect, open(\"count_vect.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899915e9-a485-4167-b3e4-db3802d3bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_ros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc584a-2d52-4b38-8484-bad55996bbc8",
   "metadata": {},
   "source": [
    "### Checking Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad22b89-bd57-407e-bd91-c85c67700e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d150d38-db40-4511-9c89-86c143d60793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b106ad7-6342-438b-be56-abc2add2be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad060a-8083-49f7-9c51-55be4f2c323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dd4e7-abcf-4764-beb7-668f08b3bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0aa25a-af24-40b9-b8a8-3c6cb3413124",
   "metadata": {},
   "source": [
    "## Model Neural Network (MLP Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab9a3c-fad6-43ac-81c6-631837ed9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1d732-4c68-42a4-ae0c-25b57939ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = MLPClassifier( early_stopping=True, validation_fraction=0.25)\n",
    "model_MLP.fit(X_train, y_train)\n",
    "\n",
    "print('Training selesai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb31d9-4a1d-4e4f-997e-af68cbb94d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_MLP, open(\"Model_MLP.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106caa6-e4bf-4f3c-96ac-073a6d97fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_MLP.loss_curve_, label='loss curve')\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7526d8-6c17-4e37-9b53-0f64d27d7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation score\n",
    "plt.plot(model_MLP.validation_scores_, label= 'validation score')\n",
    "plt.title(\"Validation Score\", fontsize=14)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35e1b8-f43b-4c49-a2be-2b31fd955c34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84479360-e60e-4a05-ad84-373747289932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641ddd3-0049-4b9c-b33e-21b16a6ff05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)],\n",
    "#     'max_iter': [50, 100, 200],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive'],\n",
    "#     'early_stopping': [True],\n",
    "#     'validation_fraction': [0.25]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75958293-ea79-47d8-919d-e9b80da5c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(model_MLP, param_grid, n_jobs= -1)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51bdef7-40ac-4606-a47d-485f8045787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_predictions = grid.predict(X_test) \n",
    "\n",
    "# print('Accuracy: {:.2f}'.format(accuracy_score(y_test, grid_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e011a-76d7-4781-b2d6-738412399cb3",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53b254-3e58-4668-a62b-ef5edbfb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0b29b-4237-4813-80dd-06d173ed750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle Data\n",
    "kf = KFold(n_splits=4, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9c969-aef4-40ee-b0da-f8b2efd830aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "    # ------------------------------ TRAIN DATASET ----------------------------------\n",
    "    data_train = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "    \n",
    "    #Save Train Dataset\n",
    "    # pickle.dump(clf, open(f\"X_train_{iteration}.p\", \"wb\"))\n",
    "    # pickle.dump(clf, open(f\"y_train_{iteration}.p\", \"wb\"))\n",
    "\n",
    "    # ------------------------------- TEST DATASET ----------------------------------\n",
    "    data_test = X[data[1]]\n",
    "    target_test = y[data[1]]\n",
    "\n",
    "    #Save Test Dataset\n",
    "    # pickle.dump(clf, open(f\"X_test_{iteration}.p\", \"wb\"))\n",
    "    # pickle.dump(clf, open(f\"y_test_{iteration}.p\", \"wb\"))\n",
    "\n",
    "    # ----------------------------------- MODEL -------------------------------------\n",
    "    clf = MLPClassifier(early_stopping=True)\n",
    "    clf.fit(data_train, target_train)\n",
    "\n",
    "    #save every model\n",
    "    # pickle.dump(clf, open(f\"model_MLP_{iteration}.p\", \"wb\"))\n",
    "\n",
    "    # ----------------------------- PREDICTION (TEST) -------------------------------\n",
    "    preds = clf.predict(data_test)\n",
    "    accuracy = accuracy_score(target_test, preds)\n",
    "\n",
    "    # ----------------------------- PRINT REPORT ------------------------------------\n",
    "    print(\"Training ke\", iteration)\n",
    "    print(classification_report(target_test, preds))\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3109cc8-3f50-4258-a4ce-5d213867e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = np.mean(accuracies)\n",
    "print(\"Rata-rata Accuracy: \", average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c3e3b-3e9b-4b50-b951-d003ac3d947f",
   "metadata": {},
   "source": [
    "## Evaluasi Akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb53e-2512-4ab2-89bc-9b0f707b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e860b-5494-4402-84da-a21bfae27374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(r\"E:\\BINAR\\Platinum-Challenge\\model_MLP.p\", \"rb\")\n",
    "# model_MLP = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d97fb-59a0-487e-8d6a-25b503e66f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model_MLP.predict(X_test)\n",
    "print(\"Testing selesai\")\n",
    "\n",
    "print(classification_report(y_test, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3926c6-6c71-4323-b571-ddc41c15998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321817ec-ca64-408d-8dd7-8ab78afffbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Matrix confusion\n",
    "cm = confusion_matrix(y_test, test, labels=model_MLP.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=model_MLP.classes_)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0de47d-c609-48ed-b279-5bcc99d2e81e",
   "metadata": {},
   "source": [
    "# Implement Model to New Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ff958-e61c-4c51-87d3-c0ead45dbe08",
   "metadata": {},
   "source": [
    "## Read the data (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bd621-9146-43d9-bcff-29f6b7650622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57dc2b-287c-426c-9396-0a953a4aee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"E:\\BINAR\\Binar-Gold-Challenge\\Dataset\\data.csv\", encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3e079-31f4-4bc7-b1ea-b869106c18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", df.shape)\n",
    "print(\"Columns are:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47262d-a60c-4818-b2b8-ec267ee695b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet= pd.DataFrame(df[['Tweet']])\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f212ce8-2edc-4c25-beb5-aab29ff8281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138e3ee-7455-4bcb-ac20-799a2fa2d1e0",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22d76c-e662-4292-ae6d-dc61163969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59b201-689a-4dcd-b6d1-d93521e768e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fucntion to Clean tweet data\n",
    "def Clean(text):\n",
    "    #lowercase for every word\n",
    "    text = text.lower()\n",
    "\n",
    "    #Clean Pattern\n",
    "    #remove USER\n",
    "    text = re.sub(r'user', ' ', text)\n",
    "    #remove 'RT'\n",
    "    text = re.sub(r'rt', ' ', text)\n",
    "    #remove 'URL'\n",
    "    text = re.sub(r'url', ' ', text)\n",
    "    #remove HTTPS\n",
    "    text = re.sub(r'https', ' ', text)\n",
    "    #remove HTTP\n",
    "    text = re.sub(r'http', ' ', text)\n",
    "    #remove &amp\n",
    "    text = re.sub(r'&amp', ' ', text)\n",
    "\n",
    "    #Clean_Unnecessary_Character\n",
    "    #remove \\n or every word afte '\\' with space\n",
    "    text = re.sub(r'\\\\+[a-zA-Z0-9]+', ' ', text)\n",
    "    #remove text emoji\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]{2,}|:[a-zA-Z0-9]{0,}', ' ', text)\n",
    "    #remove all unnecessary character \n",
    "    text = re.sub(r'[^0-9a-zA-Z\\s]+', ' ', text)\n",
    "    #remove all number\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    #remove extra space\n",
    "    text = re.sub(r'  +', ' ', text)\n",
    "    #remove space at the start or the end of string\n",
    "    text = re.sub(r'^ +| +$', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#tokenization Function\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "#import file new_kamusalay.csv\n",
    "kamus_alay = pd.read_csv(r\"E:\\BINAR\\Binar-Gold-Challenge\\Dataset\\new_kamusalay.csv\", encoding = 'ISO-8859-1', header = None)\n",
    "kamus_alay = kamus_alay.rename(columns={0: 'kata alay', 1: 'arti kata'})\n",
    "\n",
    "#Create dictionary from kamus_alay\n",
    "kamus_alay_dict = dict(zip(kamus_alay['kata alay'], kamus_alay['arti kata']))\n",
    "\n",
    "#normalization function to convert every word tha contain 'kata alay' to 'arti kata'\n",
    "def normalization(text):\n",
    "    newlist = []\n",
    "    for word in text:\n",
    "        if word in kamus_alay_dict:\n",
    "            text = kamus_alay_dict[word]\n",
    "            newlist.append(text)\n",
    "        else:\n",
    "            text = word\n",
    "            newlist.append(text)\n",
    "    return newlist\n",
    "\n",
    "#remove stopwords\n",
    "\n",
    "#list stopword from NLTK\n",
    "stopword_list = ['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia',\n",
    "                  'ia', 'seperti', 'jika', 'sehingga', 'kembali', 'dan', 'ini', 'karena', 'kepada', 'oleh', \n",
    "                  'saat', 'sementara', 'setelah', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', \n",
    "                  'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bahwa', 'atau', 'kita', 'dengan',\n",
    "                  'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', \n",
    "                  'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', \n",
    "                  'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun',\n",
    "                  'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'seolah', 'seraya', \n",
    "                  'seterusnya', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'mari', 'nanti', 'oh', 'ok', \n",
    "                  'setiap', 'sesuatu','saja', 'toh', 'walau', 'amat', 'apalagi', 'dengan', 'bahwa', 'oleh', 'aku']\n",
    "\n",
    "stopword_list.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       'gue', 'yah', 'kayak'])\n",
    "\n",
    "stopword_list = set(stopword_list)\n",
    "\n",
    "#remove stopword function\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword_list]\n",
    "    return text\n",
    "\n",
    "#Find NUll 'String' Value \n",
    "def clean_non_existed(text):\n",
    "    if text == '':\n",
    "        return None\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "#function to run all the function\n",
    "def clean_data(text):\n",
    "    text = Clean(text)\n",
    "    text = tokenization(text)\n",
    "    text = normalization(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ' '.join(text)\n",
    "    text = clean_non_existed(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b5092-92e0-460a-9b05-12adb807dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['Tweet'] = df_tweet.Tweet.apply(clean_data)\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa8cc9-869e-4c41-ab4b-0482f4226d80",
   "metadata": {},
   "source": [
    "## Cek Duplicate & Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f4c6a-7bb7-407e-a52a-97eb3e3c0c4e",
   "metadata": {},
   "source": [
    "### Duplikat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05a4ea-c4f7-4eee-abf4-56b3b9dd62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate check\n",
    "df_tweet.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef6d5a-2768-4b16-ba73-32804af90157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df_tweet.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f3020-16a3-4cb6-8976-ef94f1c79eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e008dd-ba86-4142-b0a7-f226637f3360",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728c9dd-8e0a-4ea8-97d0-4ab0d59cf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7da08d-7a7b-4b9e-834f-8ec25e3e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df_tweet.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b036de3-1583-49ba-ae4c-d4293c43837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea391b37-6525-4f13-be08-d15d11359f57",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84237b-2503-4605-a4a1-7177ce4f1031",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adea18-44d0-47b4-961d-61c5ffa14417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbb5c7-e32b-4e7c-a153-cf46b3da1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"E:\\BINAR\\Platinum-Challenge\\Neural_Netork_MLP\\Model_MLP.p\", \"rb\")\n",
    "model_MLP = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b632e-3706-4636-b970-e6e8bc2c031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"E:\\BINAR\\Platinum-Challenge\\Neural_Netork_MLP\\count_vect.p\", \"rb\")\n",
    "vectorizer = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b92cb2-0133-4d53-b695-c13e708e94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(df):\n",
    "    Data = df.Tweet.tolist()\n",
    "    count_v = vectorizer.transform(Data)\n",
    "    label = model_MLP.predict(count_v)\n",
    "\n",
    "    jsonList = []\n",
    "    for i in range(len(Data)):\n",
    "        jsonList.append({\"Tweet\" : Data[i], \"Label\" : label[i]})\n",
    "\n",
    "    json_akhir = json.dumps(jsonList, indent = 1)\n",
    "\n",
    "    return json_akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa19e7-a59c-450d-a1ad-3e7504c7d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP(df_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73cb1d-63e6-4a56-96f0-3046f9b2943c",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb6d21-b38f-40a1-bbc4-4df614361ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df_tweet.Tweet.tolist()\n",
    "count_v = vectorizer.transform(Data)\n",
    "label = model_MLP.predict(count_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3193d-782d-4b17-9587-493ec1566c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonList = []\n",
    "for i in range(len(Data)):\n",
    "    jsonList.append({\"Tweet\" : Data[i], \"Label\" : label[i]})\n",
    "\n",
    "json_akhir = json.dumps(jsonList, indent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d832c-f060-49ee-be19-43f9dc1cdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08904cf7-9c9f-4e1f-9cfb-485a79e121b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Batas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ed184-ee91-4e2d-86e8-86a0a840c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    X = vectorizer.transform([text])\n",
    "    predict = model_MLP.predict(X)[0]\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920a943-71b7-42ff-b823-1b1ffb5d9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['label'] = df_tweet.Tweet.apply(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc540b-a3b2-42be-8b0f-134da10dd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e32f4-71cb-4e4b-ae5a-ba38e0d313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['label'] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51344c-60cf-4080-ac40-116d1c4d3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet[df_tweet['label'] == 'neutral'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6c5e0-d7b6-4680-8e04-1667e23fd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads, dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15390dbd-9bb2-4ea9-830f-f10ad755ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_tweet.to_json(orient=\"table\")\n",
    "parsed = loads(result)\n",
    "dumps(parsed, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7999fce-ab71-4dcc-84ee-250a674fd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
